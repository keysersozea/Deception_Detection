{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NesWPN9VF5P5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as TF\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DEH2VkaKF-Zc",
        "outputId": "35a91ecf-ecfb-4f89-9e26-06952c503062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.25.2)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.12.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: pytorchvideo in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.1.5.post20221221)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (12.1.0)\n",
            "Requirement already satisfied: parameterized in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.9.0)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.1.10)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.25.2)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.66.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.12.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (2.8.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install fvcore\n",
        "!pip install tqdm\n",
        "!pip install pytorchvideo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6P-v1SHGE2A",
        "outputId": "f8da0227-99c3-4597-dc42-e9c021c19d92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaMfTbBMGKOP",
        "outputId": "78ba228f-fd1d-4f10-ebec-5c68c5dd9b7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unzipping completed.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "# Define the zip file path and the destination folder\n",
        "zip_file_path = '/content/drive/MyDrive/Face_Frames2_Test.zip'\n",
        "destination_folder = '/content/'\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_folder)\n",
        "\n",
        "print(\"Unzipping completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQevn8UgF5P7"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyyzoDAJF5P8"
      },
      "outputs": [],
      "source": [
        "def create_datasets(root_dir):\n",
        "    datasets_list = []\n",
        "    for i, trial_name in enumerate(os.listdir(root_dir)):\n",
        "        trial_path = os.path.join(root_dir, trial_name)\n",
        "        if os.path.isdir(trial_path):\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),  # Resize images to a standard size\n",
        "                transforms.ToTensor()  # Convert images to PyTorch tensors\n",
        "            ])\n",
        "            dataset = datasets.ImageFolder(root=trial_path, transform=transform)\n",
        "            datasets_list.append(dataset)\n",
        "    return datasets_list\n",
        "\n",
        "# Create datasets for each trial directory\n",
        "deceptive_datasets = create_datasets('Face_Frames2_Test/Deceptive')\n",
        "truthful_datasets = create_datasets('Face_Frames2_Test/Truthful')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwMs87bOF5P8"
      },
      "outputs": [],
      "source": [
        "deceptive_dict = {}\n",
        "for index, deceptive in enumerate(deceptive_datasets):\n",
        "    # Assuming deceptive is your dataset\n",
        "    deceptive_length = len(deceptive)\n",
        "    num_samples = 30\n",
        "\n",
        "    # List to hold the new dataset\n",
        "    deceptive_data = []\n",
        "\n",
        "    for i in range(0, deceptive_length, num_samples):\n",
        "        # Get the current sequence of 30 samples\n",
        "        combined_samples = []\n",
        "        for j in range(num_samples):\n",
        "            if i + j < deceptive_length:\n",
        "                data, _ = deceptive[i + j]  # Extract the data part (ignore the label if present)\n",
        "                combined_samples.append(data)\n",
        "\n",
        "\n",
        "        # Stack the current sequence of samples along a new dimension\n",
        "        if len(combined_samples) == num_samples:  # Ensure we have exactly 30 samples\n",
        "            combined_samples = [combined_samples[0]] + combined_samples + [combined_samples[-1]]\n",
        "            combined_tensor = torch.stack(combined_samples, dim=0)\n",
        "            deceptive_data.append(combined_tensor.permute(1, 0, 2, 3))\n",
        "    deceptive_dict[index] = deceptive_data\n",
        "\n",
        "truthful_dict = {}\n",
        "for index, truthful in enumerate(truthful_datasets):\n",
        "    # Assuming deceptive is your dataset\n",
        "    truthful_length = len(truthful)\n",
        "    num_samples = 30\n",
        "\n",
        "    # List to hold the new dataset\n",
        "    truthful_data = []\n",
        "\n",
        "    for i in range(0, truthful_length, num_samples):\n",
        "        # Get the current sequence of 30 samples\n",
        "        combined_samples = []\n",
        "        for j in range(num_samples):\n",
        "            if i + j < truthful_length:\n",
        "                data, _ = truthful[i + j]  # Extract the data part (ignore the label if present)\n",
        "                combined_samples.append(data)\n",
        "\n",
        "\n",
        "        # Stack the current sequence of samples along a new dimension\n",
        "        if len(combined_samples) == num_samples:  # Ensure we have exactly 30 samples\n",
        "            combined_samples = [combined_samples[0]] + combined_samples + [combined_samples[-1]]\n",
        "            combined_tensor = torch.stack(combined_samples, dim=0)\n",
        "            truthful_data.append(combined_tensor.permute(1, 0, 2, 3))\n",
        "    truthful_dict[index] = truthful_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM9fvXS-F5P9"
      },
      "source": [
        "Each element of dict represent a video\n",
        "\n",
        "Each video is represented by a list\n",
        "\n",
        "Each element of the list is a tensor of 30 frames which is 1.5 subvideos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV7zlotaF5P-",
        "outputId": "0d19ad30-80cf-43df-ca9e-6ff201f8df45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(deceptive_dict[7])\n",
        "# Each element of dict represent a video\n",
        "# Each video is represented by a list\n",
        "# Each element of the a list is a tensor of 30 frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nDZdMW5F5QA"
      },
      "outputs": [],
      "source": [
        "model = torch.load('/content/drive/MyDrive/Models/Adam_lr00001Schedular-WDe-2.pth', map_location=torch.device('cpu'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdyIxjA4F5QA",
        "outputId": "c91c71a3-6c5f-4362-8bdd-50388326951b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv3d(\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda'\n",
        "logit_means = {}\n",
        "final_predictions = {}\n",
        "\n",
        "# Clear GPU memory before starting the loop\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i in deceptive_dict:\n",
        "      logits = []\n",
        "      for tensor in deceptive_dict[i]:\n",
        "          tensor = tensor.unsqueeze(0)\n",
        "          logit = model(tensor.to(device))  # Modelden logit çıktısını al\n",
        "          logits.append(logit)\n",
        "\n",
        "      # Logitlerin ortalamasını al\n",
        "      logit_mean = torch.mean(torch.stack(logits), dim=0)\n",
        "      logit_means[i] = logit_mean  # Ortalamayı dictionary'e ekle\n",
        "\n",
        "      final_prediction = torch.argmax(logit_mean, dim=1).cpu().numpy()\n",
        "      final_predictions[i] = final_prediction  # Store the final prediction as a numpy array\n",
        "\n",
        "deceptive_predictions = np.array(list(final_predictions.values()))\n",
        "deceptive_true = np.ones(len(deceptive_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fud-vN5KK1Co"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "logit_means = {}\n",
        "final_predictions = {}\n",
        "\n",
        "# Clear GPU memory before starting the loop\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i in truthful_dict:\n",
        "      logits = []\n",
        "      for tensor in truthful_dict[i]:\n",
        "          tensor = tensor.unsqueeze(0)\n",
        "          logit = model(tensor.to(device))  # Modelden logit çıktısını al\n",
        "          logits.append(logit)\n",
        "\n",
        "      # Logitlerin ortalamasını al\n",
        "      logit_mean = torch.mean(torch.stack(logits), dim=0)\n",
        "      logit_means[i] = logit_mean  # Ortalamayı dictionary'e ekle\n",
        "\n",
        "      final_prediction = torch.argmax(logit_mean, dim=1).cpu().numpy()\n",
        "      final_predictions[i] = final_prediction  # Store the final prediction as a numpy array\n",
        "\n",
        "truthful_predictions = np.array(list(final_predictions.values()))\n",
        "truthful_true = np.zeros(len(deceptive_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRSveZJbNctR"
      },
      "outputs": [],
      "source": [
        "predictions = np.concatenate((deceptive_predictions, truthful_predictions))\n",
        "true = np.concatenate((deceptive_true, truthful_true))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0mEX-SMPbN1",
        "outputId": "8e1a0f9d-abad-40c0-d0b5-ee5b566215aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Accuracy  F1 Score  Precision    Recall\n",
            "0  0.818182  0.818182   0.818182  0.818182\n"
          ]
        }
      ],
      "source": [
        "accuracy = accuracy_score(true, predictions)\n",
        "f1 = f1_score(true, predictions)\n",
        "precision = precision_score(true, predictions)\n",
        "recall = recall_score(true, predictions)\n",
        "df = pd.DataFrame({'Accuracy': [accuracy], 'F1 Score': [f1], 'Precision': [precision], 'Recall': [recall]})\n",
        "\n",
        "print(df)\n",
        "df.to_csv('test_Adam_lr00001Schedular-WDe-2.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9H1Xa-kX2Km"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.-1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
