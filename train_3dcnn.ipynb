{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJo21ek7wNrx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import zipfile\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as TF\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import StepLR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_wCkxnrbgt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8d33a6-3f9e-4421-cd16-c58372482af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuEq4hwIwNry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf73373-e867-45fd-8164-3d745aa1529d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.14.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement zipfile (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for zipfile\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.25.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.12.0)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=d1c5f8660941060409aebe3c6b881a43ad78eafc34d1b2b285dd5dc33b00f525\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=9fbb4353ca6ac99cf4366e43e666b7130e15e0a262c364bda1c248b5f3b32571\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Collecting pytorchvideo\n",
            "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.1.5.post20221221)\n",
            "Collecting av (from pytorchvideo)\n",
            "  Downloading av-12.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parameterized (from pytorchvideo)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (0.1.10)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (1.25.2)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (4.66.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (4.12.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorchvideo) (2.8.2)\n",
            "Building wheels for collected packages: pytorchvideo\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=ca3efc102775f40309ebb09f7ff8079de270dcee66ffb6d8ac847a58bb1ddcb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n",
            "Successfully built pytorchvideo\n",
            "Installing collected packages: parameterized, av, pytorchvideo\n",
            "Successfully installed av-12.1.0 parameterized-0.9.0 pytorchvideo-0.1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!pip install zipfile\n",
        "!pip install fvcore\n",
        "!pip install tqdm\n",
        "!pip install pytorchvideo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd6cQlQYwNr0"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "# URL of your Google Drive file\n",
        "#url = 'https://drive.google.com/uc?export=download&id=YOUR_FILE_ID'\n",
        "#url = 'https://drive.google.com/uc?export=download&id=1pW5niXoC5VAO0x8M5G48F24Gp7Ji0cyZ'\n",
        "#output = 'Face_Frames2.zip'  # Name of the file to be saved locally\n",
        "\n",
        "#gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGLCtAKHwNr1",
        "outputId": "181e6388-361c-4708-fe1c-235e6f1cc5e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping completed.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "# Define the zip file path and the destination folder\n",
        "zip_file_path = '/content/drive/MyDrive/Face_Frames2.zip'\n",
        "destination_folder = '/content/'\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination_folder)\n",
        "\n",
        "print(\"Unzipping completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxM5ecwOwNr2"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "deceptive = datasets.ImageFolder(root='/content/Face_Frames2/Deceptive', transform=transform)\n",
        "truthful = datasets.ImageFolder(root='/content/Face_Frames2/Truthful', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7VuPQCLwNr2",
        "outputId": "8703cfc5-a1df-4991-c87d-1c4b736ae47f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "deceptive[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bai9r-iwNr3"
      },
      "outputs": [],
      "source": [
        "# Assuming deceptive is your dataset\n",
        "deceptive_length = len(deceptive)\n",
        "truthful_length = len(truthful)\n",
        "num_samples = 30\n",
        "\n",
        "# List to hold the new dataset\n",
        "deceptive_data = []\n",
        "truthful_data = []\n",
        "\n",
        "for i in range(0, deceptive_length, num_samples):\n",
        "    # Get the current sequence of 30 samples\n",
        "    combined_samples = []\n",
        "    for j in range(num_samples):\n",
        "        if i + j < truthful_length:\n",
        "            data, _ = deceptive[i + j]  # Extract the data part (ignore the label if present)\n",
        "            combined_samples.append(data)\n",
        "\n",
        "    # Stack the current sequence of samples along a new dimension\n",
        "    if len(combined_samples) == num_samples:  # Ensure we have exactly 30 samples\n",
        "        combined_samples = [combined_samples[0]] + combined_samples + [combined_samples[-1]]\n",
        "        combined_tensor = torch.stack(combined_samples, dim=0)\n",
        "        deceptive_data.append(combined_tensor)\n",
        "\n",
        "for i in range(0, truthful_length, num_samples):\n",
        "    # Get the current sequence of 30 samples\n",
        "    combined_samples = []\n",
        "    for j in range(num_samples):\n",
        "        if i + j < truthful_length:\n",
        "            data, _ = truthful[i + j]  # Extract the data part (ignore the label if present)\n",
        "            combined_samples.append(data)\n",
        "\n",
        "    # Stack the current sequence of samples along a new dimension\n",
        "    if len(combined_samples) == num_samples:  # Ensure we have exactly 30 samples\n",
        "        combined_samples = [combined_samples[0]] + combined_samples + [combined_samples[-1]]\n",
        "        combined_tensor = torch.stack(combined_samples, dim=0)\n",
        "        truthful_data.append(combined_tensor)\n",
        "\n",
        "truthful_len = len(truthful_data)\n",
        "deceptive_len = len(deceptive_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP3In-WjwNr3",
        "outputId": "5fd02817-39e1-4486-8e8a-a4056acef0d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "825"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "truthful_data[0].shape\n",
        "truthful_len\n",
        "deceptive_data[0].shape\n",
        "deceptive_len\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-mQRBOswNr3"
      },
      "outputs": [],
      "source": [
        "total_data = truthful_data + deceptive_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blVJFJ62wNr4"
      },
      "outputs": [],
      "source": [
        "# Change 1st and 2nd dimensions of each sample\n",
        "new_dataset = [sample.permute(1, 0, 2, 3) for sample in total_data]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_ro_MSbwNr4"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_list, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_list (list): List of tensors, each of shape [30, 3, 224, 224].\n",
        "            labels (list): List of labels corresponding to each tensor.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.data_list = data_list\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data_list[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        return data, label\n",
        "\n",
        "\n",
        "# Assuming labels is a list of labels corresponding to each sample\n",
        "labels = [0] * truthful_len + [1] * deceptive_len\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "# Create an instance of the custom dataset with labels and transform\n",
        "#train_dataset = CustomDataset(new_dataset, labels, transform=None)\n",
        "#train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "# Save the dataset\n",
        "#torch.save(train_dataset, 'train_dataset.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Test Split using sklearn\n",
        "X = new_dataset\n",
        "y = labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "test_dataset = CustomDataset(X_test, y_test, transform=None)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "train_dataset = CustomDataset(X_train, y_train, transform=None)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "V2qwlKIMxBRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OKsP7jqwNr4",
        "outputId": "348a9da5-95bc-4482-9c8e-34a19b7dc457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        }
      ],
      "source": [
        "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\n",
        "model.blocks[-1].output_pool = nn.Identity()\n",
        "model.blocks[-1].proj = nn.Linear(2048, 2, bias=True)\n",
        "for i in range(len(model.blocks) - 2):\n",
        "    model.blocks[i].requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtk52uFOUOZc"
      },
      "outputs": [],
      "source": [
        "def metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return dict(\n",
        "        accuracy=accuracy)\n",
        "\n",
        "\n",
        "def train_epoch(model, optimizer, criterion, dataloader, device):\n",
        "    model.train()\n",
        "    losses = list()\n",
        "    y_true_list = list()\n",
        "    y_pred_list = list()\n",
        "    batches = list()\n",
        "    with tqdm(enumerate(dataloader), total=len(dataloader), leave=True) as iterator:\n",
        "        for idx, batch in iterator:\n",
        "            optimizer.zero_grad()\n",
        "            x, y_true= batch\n",
        "            prediction = model.forward(x.to(device))\n",
        "            loss = criterion(prediction, y_true.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            iterator.set_description(f\"train loss={loss:.2f}\")\n",
        "            losses.append(loss)\n",
        "            #y_true_list.append(y_true)\n",
        "            #y_pred_list.append(prediction.argmax(-1))\n",
        "            batches.append(idx)\n",
        "    return torch.stack(losses), torch.cat(y_true_list), torch.cat(y_pred_list)\n",
        "\n",
        "\n",
        "\n",
        "def test_epoch(model, criterion, dataloader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        losses = list()\n",
        "        y_true_list = list()\n",
        "        y_pred_list = list()\n",
        "        field_ids_list = list()\n",
        "        batches = list()\n",
        "        with tqdm(enumerate(dataloader), total=len(dataloader), leave=True) as iterator:\n",
        "            for idx, batch in iterator:\n",
        "                x, y_true = batch\n",
        "                logits = model.forward(x.to(device))\n",
        "                loss = criterion(logits, y_true.to(device))\n",
        "                iterator.set_description(f\"test loss={loss:.2f}\")\n",
        "                losses.append(loss)\n",
        "                #y_true_list.append(y_true)\n",
        "                #y_pred_list.append(logits.argmax(-1))\n",
        "                batches.append(idx)\n",
        "        return  torch.stack(losses), torch.cat(y_true_list), torch.cat(y_pred_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On7qCGTK1Z7E"
      },
      "outputs": [],
      "source": [
        "#device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), 0.0001, weight_decay=0.01)\n",
        "#scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "log = list()\n",
        "best_loss = float('inf')\n",
        "patience = 3\n",
        "n_epoch = 1\n",
        "for epoch in range(n_epoch):\n",
        "    train_loss, train_true, train_pred = train_epoch(model, optimizer, criterion, train_dataloader, device)\n",
        "    test_loss, test_true, test_pred = test_epoch(model, criterion, test_dataloader, device)\n",
        "    scores1 = metrics(test_true.cpu(), test_pred.cpu())\n",
        "    scores= metrics(train_true.cpu(), train_pred.cpu())\n",
        "    scores_msg = \", \".join([f\"{k}={v:.2f}\" for (k, v) in scores.items()])\n",
        "    scores_msg = \", \".join([f\"{k}={v:.2f}\" for (k, v) in scores1.items()])\n",
        "    test_loss = test_loss.cpu().detach().numpy().mean()\n",
        "    train_loss = train_loss.cpu().detach().numpy().mean()\n",
        "    #scheduler.step()\n",
        "\n",
        "    #print(f\"epoch {epoch}: trainloss {train_loss:.2f}\" + scores_msg)\n",
        "    print(f\"epoch {epoch}: trainloss {train_loss:.2f}, testloss {test_loss:.2f} \" + scores_msg)\n",
        "\n",
        "    scores1[\"epoch\"] = epoch\n",
        "    scores1[\"trainloss\"] = train_loss\n",
        "    scores1[\"testloss\"] = test_loss\n",
        "    log.append(scores1)\n",
        "\n",
        "    # Early stopping check\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping triggered at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "\n",
        "    log_df = pd.DataFrame(log).set_index(\"epoch\")\n",
        "    #log_df.to_csv(\"Adam_lr0001-WDe-2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIq-nYS91eQA"
      },
      "outputs": [],
      "source": [
        "name = '/content/drive/MyDrive/Models/Adam_lr00001Schedular-WDe-2.pth'\n",
        "torch.save(model, name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "va8g6ShqyW-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcfoJsuPUSdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2842a3-0b8c-46c8-e15b-3f2b56a40dde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9446064139941691"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "\"\"\"# Load the model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_deneme = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=False)\n",
        "#model_deneme.load_state_dict(torch.load('lr0001-WDe-3_nosplit.pth', map_location=device))\n",
        "model_deneme = torch.load(\"/content/drive/MyDrive/lr0001-WDe-3_nosplit.pth\", map_location=device)\n",
        "model_deneme.to(device)\n",
        "model_deneme.eval()\"\"\"\n",
        "device = 'cuda'\n",
        "#model = torch.load('/content/drive/MyDrive/Adam_lr0001-WDe-2.pth', map_location=torch.device('cpu'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "# Get predictions\n",
        "y_pred = []\n",
        "y_true = []\n",
        "for batch, (X, y) in enumerate(test_dataloader):\n",
        "    y_true.append(y)\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    y_pred.append(model(X).argmax(-1).cpu().numpy())\n",
        "\n",
        "y_true = np.concatenate(y_true)\n",
        "y_pred = np.concatenate(y_pred)\n",
        "accuracy_score(y_true, y_pred)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}